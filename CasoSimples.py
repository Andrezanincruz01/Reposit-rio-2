# -*- coding: utf-8 -*-
"""CASO SIMPLES .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UPJTwxgFASVR3piborMJe6GJfm_KQlMQ
"""

#===============================================
# Código - CASO SIMPLES - 01/07 - AL-Quad
#===============================================
# Este script resolve o problema de otimização de portfólio multi‑objetivo
# para restrição de cardinalidade relaxada, usando o Lagrangeano Aumentado
# com penalidade quadrática (AL‑Quad).  Foram adicionados
# comentários em português para explicar o papel de cada função.

# Pré‑requisitos e montagem do Drive
from google.colab import drive  # Permite gravar e ler arquivos no Google Drive

drive.mount('/content/drive')# cria o drive em /content/drive  – só usado no Google Colab

# =============================================================================
# Instalação de dependências (execute no terminal, não dentro do script):
#  As linhas (!pip) abaixo servem apenas de lembrete do que precisa estar instalado.
!pip install numpy scipy matplotlib pandas cplex docplex
!pip install pygmo
# =============================================================================

# ======================Bibliotecas básicas ===================================
import numpy as np                     # Álgebra vetorial para pythom
from scipy.optimize import minimize    # SLSQP para sub‑problema de PGMMOP
import matplotlib.pyplot as plt        # Criação de gráficos
import time                            # Mensuração de tempo
import pandas as pd                    # Tabelas
import requests                        # Download de datasets
from docplex.mp.model import Model     # CPLEX/DOcplex p/ resolver MIQP
import pygmo as pg                     # Métricas multi‑objetivo (hipervolume)
# ============================================================================
# DADOS DO PORTFÓLIO
# ============================================================================

n = 6  #Dimensão do problema
v = np.array([0.021, 0.04, -0.034, -0.028, -0.005, 0.006]) #Vetor de retorno esperado
Q = np.array([
    [0.038, 0.020, 0.017, 0.014, 0.019, 0.017],
    [0.020, 0.043, 0.015, 0.013, 0.021, 0.014],
    [0.017, 0.015, 0.034, 0.011, 0.014, 0.014],
    [0.014, 0.013, 0.011, 0.044, 0.014, 0.011],
    [0.019, 0.021, 0.014, 0.014, 0.040, 0.014],
    [0.017, 0.014, 0.014, 0.011, 0.014, 0.046]
]) # Matriz de covariância já dada.
print("Dimensão do problema (n):", n)
print("\nVetor de retornos esperados (v):")
print(v)
print("\nMatriz de covariância (Q):")
print(Q)

colors = {1: 'red', 2: 'green', 3: 'orange', 4: 'magenta', 5: 'cyan', 6: 'black'}

# ============================================
# FUNÇÕES AUXILIARES
# ============================================

def tau_Rayleigh(Q):                # Usado para determinar determinar o primeiro parâmetro de penalização
    ones_vec = np.ones(Q.shape[0])
    t = Q @ ones_vec
    return (t.T @ (Q @ t)) / (t.T @ t + 1e-12)

def cardinalidade(x):               # Verifica a cardinalidade do vetor encontrado, se o vetor tem tamanho x>=1e-4 ele é contado em 1
    return np.sum(x >= 1e-4)

def binario(yk):                               # Verifica se o vetor y obtido se tornou realmente binário ao contar quantas
    return np.sum((yk >= 1e-14) & (yk != 1.0))# componentes ficaram no "meio" entre 0 e 1

def proj_box(x, low, up):                      # Usado para projeção na caixa, cortando as componentes do vetor x para que fique dentro dos limites
    return np.minimum(np.maximum(x, low), up)

def proj_hiperplano(x, w, cte):
    return x + ((cte - np.dot(w, x)) / (np.dot(w, w) + 1e-12)) * w    #Aplica em x a projeção no hiperplano definido por w^Tz=cte

def proj_halfspace(x, w, cte):                                        # projeção sobre o semiespaço definido por w^Tz >= cte, se wx>=cte não faz nada
    if np.dot(w, x) < cte:                                            # Se wx< cte, então o vetor é puxado para o hiperplano
        return x + ((cte - np.dot(w, x)) / (np.dot(w, w) + 1e-12)) * w
    else:
        return x

# ============================================
# MÉTODO DE DYKSTRA
# Transcrito para python do trabalho de Krejic (2023)
# ============================================

def dykstra(x, y, ro, alfa, up, low, v, maxcycles=500, tol=1e-12):
    n_local = len(x)
    e_local = np.ones(n_local)
    dk2 = np.zeros(n_local); dk3 = np.zeros(n_local)
    dk5 = np.zeros(n_local); dk6 = np.zeros(n_local)
    err = 2.0; k = 0
    while err >= tol and k < maxcycles:
        T = 0.0
        # hiperplano soma(x)=1
        z = x - dk2
        x1 = proj_hiperplano(z, e_local, 1.0)
        t = x1 - z; T += np.linalg.norm(t - dk2)**2; dk2 = t
        # caixa x
        z = x1 - dk3
        x2 = proj_box(z, low, up)
        t = x2 - z; T += np.linalg.norm(t - dk3)**2; dk3 = t
        # semiespaço soma(y)>=n-alfa
        z = y - dk5
        y1 = proj_halfspace(z, e_local, n_local - alfa)
        t = y1 - z; T += np.linalg.norm(t - dk5)**2; dk5 = t
        # caixa y
        z = y1 - dk6
        y2 = proj_box(z, low, up)
        t = y2 - z; T += np.linalg.norm(t - dk6)**2; dk6 = t
        err = np.sqrt(T)
        x, y = x2, y2
        k += 1
    return x, y, err

def proj(z, v, low, up, ro, alfa):     # Função para chamar a projeção de Dykstra
    x = z[:n]; y = z[n:]
    xp, yp, _ = dykstra(x, y, ro, alfa, up, low, v)
    return np.concatenate([xp, yp])

def verifica_viabilidade(z, n, alfa, tol=1e-8): # Função que criei para verificar a viabilidade ao longo das iterações
    x = z[:n]; y = z[n:]
    ok = True; msgs = []
    if abs(x.sum() - 1.0) > tol:
        ok = False;
        msgs.append(f"Soma x={x.sum():.6f}≠1")
    if not (np.all(x>=-tol) and np.all(x<=1+tol)):
        ok = False;
        msgs.append("x fora [0,1]")
    if not (np.all(y>=-tol) and np.all(y<=1+tol)):
        ok = False;
        msgs.append("y fora [0,1]")
    if y.sum() < (n - alfa - tol):
        ok = False;
        msgs.append(f"Soma y={y.sum():.6f}<{n-alfa}")
    return ok, msgs

# ==============================================================
# Lagrangeano aumentado com penalização quadrática + PGM interno
# ==============================================================
def F_mobj(z):
    """
    Retorna F(z)=(0.5xQx,-vx)
    """
    x = z[:n]
    return np.array([0.5 * x@(Q@x), -v@x])

def lagrangiano_penalizado(z, tau, lam):
    x, y = z[:n], z[n:]
    base = F_mobj(z)
    # termo: (tau/2)*||x0y + lam/tau||² multiplicado pelo vetor e
    term = (tau / 2) * np.linalg.norm(x * y + lam / tau)**2
    return base + term * np.ones_like(base)

def jacobian_lagrangiano_penalizado(z, tau, lam):
    """
    Retorna uma matriz 2×(2n) onde cada linha i é o gradiente de L_i em relação a [x;y].
    """
    x, y = z[:n], z[n:]
    # gradiente de F_mobj
    grad1_x = Q @ x
    grad2_x = -v
    # termo de penalidade
    term = x * y + lam / tau
    # gradientes do termo de penalidade
    pen_x = tau * (term * y)
    pen_y = tau * (term * x)
    # monta as duas linhas
    g1 = np.concatenate([grad1_x + pen_x, pen_y])
    g2 = np.concatenate([grad2_x + pen_x, pen_y])
    return np.vstack([g1, g2])


def Armijo_tamanho_passo(z, d, F, J, A_k, delta):
    """
    Seleciona tamanho de passo t através do critério de Armijo não-monótono do tipo máximo..
    Parâmetros:
    z : Ponto atual (x,y)
    d : Direção de busca obtida pelo SLSQP
    F : Função objetivo penalizada (na prática "lagrangiano_penalizado"), que retorna vetor de dimensão 2.
    J : Avaliação do gradiente em z na direção d, isto é [∇L₁·d, ∇L₂·d].
    A_k : Par (A₁, A₂) acumulado nas últimas iterações.
    delta : Parâmetro usado apenas em Armijo.

    Retorna: O maior tamanho de passo t=1/2^j (j = 0, 1, ...,19) que satisfaz a cond. de Armijo
        F(z + td) ≤ A_k + δ t J;
        retorna 1.0 se nenhum j satisfizer.
    """
    for j in range(20):
        t_val = 1/(2**j)
        z_next = z + t_val*d
        F_next = np.array(F(z_next))
        rhs = np.array(A_k) + delta*t_val*J
        if np.all(F_next <= rhs):
            return t_val
    return 1

def compute_A_k(f_values, k, M):
    """
    Calcula o valor de A_k para Armijo não-monótono.

    Parâmetros usados:
    f_values : Histórico de valores de F(z) (cada um sendo um vetor de dimensão 2).
    k : índice da iteração atual.
    M : Número máximo de iterações consideradas em A_k (M=10).

    Retorna:
    A_k : Par (A₁, A₂), onde cada componente é o máximo de F_j nas últimas min(k, M) iterações.
    """
    m_k = min(k, M)
    recentes = f_values[max(0, k-m_k) : k+1] #Seleciona o conjunto de valores F(z^j) desde j = k–m_k até j = k
    if not recentes:  # Caso k=0 e f_values tem só um elemento
        return f_values[-1]
    A1 = max(f[0] for f in recentes)
    A2 = max(f[1] for f in recentes)
    return (A1, A2)

def processo_interno_PGMMOP(z, tau, lam, alpha, max_iter=50,M=10, delta=0.5, tol_inner=1e-08):
    """
    Usado para executar ométodo PGMMOP para o Lagrangiano aumentado.
    Parâmetros:
    z : Ponto inicial [x,y].
    tau : Parâmetro de penalização.
    lam : Vetor com Multiplicadores de Lagrange.
    alpha : Cardinalidade trabalhada.
    max_iter : Número máximo de iterações no processo interno.
    M : Usado em compute_A_k.
    delta : Usado na condição de Armijo.
    tol_inner: Tolerância para critério de parada interno (tol_1).

    Retorna:
    z_new : Ponto final (x,y).
    d : Última direção de busca encontrada.
    k : Número de iterações internas utilizadas.
    Fzp :Valor de Lagrangiano_pen(z) durante o processo.
    Fz_new : valor de F(z) ao final do processo.

    res=minimize é a função que utiliza SLSQP para encontrar a direção de busca com base nas restrições presentes em "cons"
    """
    f_vals = []
    for k in range(max_iter):
        Fzp = lagrangiano_penalizado(z, tau, lam)
        f_vals.append(Fzp)
        d0 = np.zeros_like(z)

      # Definimos os vetores gradientes em relação a x e y da função lagrangiana:
        x, y = z[:n], z[n:]
        b1 = np.concatenate([Q@x + lam*y + tau*(x*(y**2)), lam*x + tau*((x**2)*y)])
        b2 = np.concatenate([-v + lam*y + tau*(x*(y**2)), lam*x + tau*((x**2)*y)])

        # variável inicial do SLSQP: d concatenado com t=0:
        dt0 = np.concatenate([np.zeros_like(z), [0.0]])

        # função objetivo suave para determinar direção de busca
        def obj_suave(dt):
            d = dt[:-1]
            t = dt[-1]
            return t + 0.5 * np.dot(d, d)

        # Novo gradiente objetivo suave
        def obj_suave_grad(dt):
            d = dt[:-1]
            grad_d = d
            grad_t = 1.0
            return np.concatenate([grad_d, [grad_t]])

        # Restrições do problema original mais duas restrições novas para suavizar o máximo na busca de direção
        # Mais informações sobre o minimize e sua construção são encontradas no trabalho.
        # Os "lambda dt: <expressão com dt>" significa que criamos na hora uma função anonima com variação dt
        # Por exemplo, "lambda dt: np.sum(z[:n]+dt[:n]) - 1" seria c(dt)=sum(z_i+dt_i)-=0
        cons = [
            {"type":"eq",   "fun": lambda dt: np.sum(z[:n]+dt[:n]) - 1},
            {"type":"ineq", "fun": lambda dt: z[:n]+dt[:n]},
            {"type":"ineq", "fun": lambda dt: 1-(z[:n]+dt[:n])},
            {"type":"ineq", "fun": lambda dt: z[n:]+dt[n:2*n]},
            {"type":"ineq", "fun": lambda dt: 1-(z[n:]+dt[n:2*n])},
            {"type":"ineq", "fun": lambda dt: np.sum(z[n:]+dt[n:2*n]) - (n-alpha)},
            # restrições do suavização do máximo
            {'type':'ineq', 'fun': lambda dt: dt[-1] - np.dot(b1, dt[:-1])},
            {'type':'ineq', 'fun': lambda dt: dt[-1] - np.dot(b2, dt[:-1])}
        ]

        # Resolve o problema suave com SLSQP
        res = minimize(obj_suave, dt0, method='SLSQP',
                      jac=obj_suave_grad,
                      constraints=cons,
                      options={'ftol':1e-06, 'maxiter':500, 'disp':False})

        # Recupera a direção ótima d (descarta o valor ótimo t)
        d = res.x[:-1]
        Ak = compute_A_k(f_vals, k, M)
        Jv = np.array([
            jacobian_lagrangiano_penalizado(z,tau,lam)[0]@d,
            jacobian_lagrangiano_penalizado(z,tau,lam)[1]@d])
        a = Armijo_tamanho_passo(z, d, lambda zz: lagrangiano_penalizado(zz, tau, lam),
                                      Jv, Ak, delta)
        z_new = z + a*d
        z_new = proj(z_new, v, 0, 1, 0, alpha)
        if np.linalg.norm(d) <= tol_inner:
            return z_new, d, k, F_mobj(z_new)
        z = z_new
    return z, d, k, F_mobj(z_new)

# ============================================
# Processo externo da penalização
# ============================================

def processo_externo_penalizacao(z0, alpha, max_outer=50, tol_stop=1e-8):
    """
    Executa o loop externo de atualização de τ e λ para o Lagrangiano aumentado.

    Parâmetros:
    z0 : Ponto de partida [x, y].
    alpha : Cardinalidade trabalhada.
    max_outer : Quantidade máxima de iterações externas.
    tol_stop : Tolerância do critério de parada externo (tol_2).

    Retorna :
    z_new :Solução final.
    d : Última direção de busca interna.
    iter_externa : Quantidade de iterações externas usadas.
    Fz : Valor de F(z) da última iteração interna
    tau : Último parâmetro de penalização.
    norm_diff : Diferença ||F(z_new) – F_prev|| usada também no critério de parada.
    lam : Multiplicadores de Lagrange finais.
    t_ext : Tempo total usado nas iterações externas (em segundos).
    t_int : tempo total usado nas iterações internas (em segundos).
    """
    z = z0.copy()
    tau = tau_Rayleigh(Q)
    lam = np.zeros(n)
    lam_min, lam_max = -1e4, 1e4
    tau_max = 1e4
    eta_val = 10
    t_ext = t_int = 0.0

    # calcula F na partida e a norma do produto de Hadamard inicial
    F_prev = F_mobj(z)
    normHad_prev = np.linalg.norm(z[:n] * z[n:])

    for k in range(max_outer):
        t0 = time.time()
        # Usa o processo interno
        z_new, d, it_int, Fz = processo_interno_PGMMOP(z, tau, lam, alpha)
        t1 = time.time()
        t_int += (t1 - t0)

        x_new, y_new = z_new[:n], z_new[n:]
        norm_diff = np.linalg.norm(Fz - F_prev)
        normHad = np.linalg.norm(x_new * y_new)

        print(f"Iter. ex: {k} (it. int = {it_int}, ||d|| = {np.linalg.norm(d):.2e}): "
              f"tau = {tau:.7f}, lambda = {lam}, normHad = {normHad:.2e}, ΔF = {norm_diff:.2e}")

        # Critério de parada externo presente no trabalho
        cond1 = normHad**2 <= tol_stop*(np.linalg.norm(x_new)**2 * np.linalg.norm(y_new)**2)
        cond2 = norm_diff <= tol_stop
        if cond1 and cond2:
            t_ext += time.time() - t0
            print("Critério externo atingido.")
            return z_new, d, k+1, Fz, tau, norm_diff, lam, t_ext, t_int

        # Atualiza o vetor lam projetando em [lam_min, lam_max]^n
        lam = np.clip(lam + tau*(x_new * y_new), lam_min, lam_max)
        # Atualiza tau conforme regra apresentada nas observações do trabalho
        if normHad <= (1/eta_val) * normHad_prev:
            tau_new = tau
        else:
            eta_val += ((n - alpha)/(2*n))*(abs(v @ x_new) / (np.sqrt(x_new @ (Q @ x_new)) + 1e-12))
            tau_new = eta_val * tau
        tau = min(tau_new, tau_max)

        # Prepara próxima iteração
        F_prev, normHad_prev, z = Fz, normHad, z_new
        t_ext += time.time() - t0

    # Caso atinja o máximo de iterações programadas, retorna o seguinte
    return z, d, max_outer, Fz, tau, norm_diff, lam, t_ext, t_int

# ====================================================
# Parâmetros do multi-start
# ====================================================
n_starts     = 300
alpha_values = [1,2,4,6]

# ============================================
# GERAÇÃO DOS PONTOS INICIAIS (ESSA GERAÇÃO USANDO RAW_INITS DE PONTOS
# INICIAIS É USADA SOMENTE PARA APROXIMAÇÃO DA FRONTEIRA DE PARETO NOS PROBLEMAS
# DE N=6,31 PARA OBTER PONTOS NO CONJUNTO VIÁVEL APÓS DYKSTRA
# PARA PORT2, PORT3 e PORT4 USAMOS UMA GERAÇÃO ALEATÓRIA COM MESMA SEED
# ============================================
np.random.seed(42)
raw_inits = []
z_centro = np.concatenate([np.full(n, 1/n), np.zeros(n)])
# =====================================================================================
# PROJEÇÃO dos pontos iniciais de cada partida
# raw_z  = ponto bruto (Inviável, gerado a partir de z_centro e direção aleatória)
# z0     = Defino como raw_z
# proj_z = ponto viável inicial (x,y) por Dykstra usado no multi-start
# =====================================================================================
for _ in range(n_starts):
    # direção aleatória
    d = np.random.randn(2 * n)
    d /= np.linalg.norm(d)
    # raio uniforme em [0,1]
    rho = np.random.uniform(0, 10)
    # candidato sem projeção ainda
    raw_inits.append(z_centro + rho * d)

# ============================================
# Chamada principal: Loop por α (MULTI-START)
# ============================================
# Dicionários vázios usados para guardarem informações para cada alpha em alpha_values
all_solutions = {};  # "all_solutions" lista dos vetores ótimos z* encontrados para cada start
all_F = {};          # "all_F" lista dos valores de F(z*) (o vetor de objetivos)
all_tau = {};        # "all_tau" lista de valores finais de τ
all_diff = {};       # "all_diff" lista de valores ΔF (mudança de F)
all_iter = {};       # "all_iter" lista de números de iterações externas usadas
all_time = {};       # "all_time" lista de tempos (s) gastos em cada run
all_ND = {};         # "all_ND" lista de listas de soluções não‑dominadas encontradas
nd_points = {};      # "nd_points" lista das frentes não‑dominadas F(z*)=(f₁(z*),f₂(z*))

start_total = time.perf_counter()

for alpha in alpha_values:
    print(f"\n===== Executando para α = {alpha} =====")

# Listas temporárias para acumular os resultados de cada start
    sols = [];        # z* de cada start
    Fs = [];          # F(z*) de cada start
    taus = [];        # tau final de cada start
    diffs = [];       # ||ΔF|| de cada start
    iters_e = [];     # número de iters externas de cada start
    times_e = [];     # tempo de cada start
    ND = [];          # lista de (z*,F_orig) não-dominados parciais

    proj_inits = []
    for raw_z in raw_inits:
        z0 = raw_z.copy()
        proj_z = proj(z0, v, low=0, up=1, ro=0, alfa=alpha)
        proj_inits.append(proj_z)

    # =======================================================
    #  MULTI-START: roda o AL+PGM-MOP a partir de cada proj_z
    # =======================================================
    for i, z0 in enumerate(proj_inits, 1): #(i=1,2,3,...) para não começar i=0
        print(f"\n--- Início {i} para α = {alpha} ---")

        (z_opt, d, it_ex, Fz, tau_f,
         diff_f, lam_f, t_ext, t_int) = processo_externo_penalizacao(z0, alpha)

        # armazena resultados
        sols .append(z_opt)     # acrescenta o z* encontrado na lista
        Fs   .append(Fz)        # acrescenta o F(z*) encontrado
        taus .append(tau_f)     # acrescente o tau encontrado
        diffs.append(diff_f)    # acrescenta o ||ΔF||
        iters_e.append(it_ex)   # acrescenta o número final de interações exteriores
        times_e.append(t_ext)   # acrescenta o tempo ext
        # impressão de diagnóstico
        print("Ponto inicial (projetado):", np.round(z0, 4))
        print("Ponto ótimo z*           :", np.round(z_opt, 4))
        vi, msgs = verifica_viabilidade(z_opt, n, alpha)
        print("VIÁVEL" if vi else f"NÃO VIÁVEL: {msgs}")

        x_f, y_f = z_opt[:n], z_opt[n:]
        print(f"Retorno {v @ x_f:.6f}, Risco {np.sqrt(x_f @ (Q @ x_f)):.6f}, "
              f"Card {cardinalidade(x_f)}, Dif_y {binario(y_f)}")

        # ===================================================
        #Atualiza lista de não-dominados (ND)
        # ===================================================
        # Calculamos os objetivos originais (f_1,f_2) para o ponto ótimo atual z_opt
        F_orig = F_mobj(z_opt)          # F(z_opt)=[f1(z_opt),f2(z_opt)]
        dominado = False               # Iniciamos tomando "dominado" como False (para indicar se o ponto ótimo é dominado)
        new_ND = []                    # lista auxiliar vazia para armazenar os pontos não dominados.
        # primeiro laço, verificamos se z_opt é dominado.
        for zp, Fp in ND: #Percorre todos os pontos da lista ND (não dominados existentes) e verifica se z_opt é dominado por algum ponto já em ND.
            # Fp <= F_orig é um vetor booleano (True ou False) com o resultado dessa comparação em todos (np.all) os componentes de f1 e f2.
            # Fp < F_orig cria outro vetor booleano agora para menor estrito e np.any verifica se existe ao menos uma componente com essa desigualdade.
            # and liga essas duas questões necessárias para a definição clássica para que zp seja considerado dominado.
            if np.all(Fp <= F_orig) and np.any(Fp < F_orig):
               # Se zp domina z_opt, então z_opt é dominado e podemos parar.
                dominado = True
                break
         # segundo laço, remoção dos pontos dominados e atualização da lista.
        for zp, Fp in ND: #remove do ND pontos que agora são dominados por z_opt
            if not (np.all(F_orig <= Fp) and np.any(F_orig < Fp)):
                new_ND.append((zp, Fp)) # Se zp não é dominado por F_orig, mantemos zp na lista
        if not dominado: #se z_opt não for dominado, adicionamos ele na lista de não dominados
            new_ND.append((z_opt, F_orig))
         # Atualiza a lista de não-dominados.
        ND = new_ND


    # guarda os resultados para cada alpha
    all_solutions[alpha] = sols
    all_F        [alpha] = Fs
    all_tau      [alpha] = taus
    all_diff     [alpha] = diffs
    all_iter     [alpha] = iters_e
    all_time     [alpha] = times_e
    all_ND       [alpha] = ND

    # Fronteira não dominada que será levada para o notebook de perfis de desempenho
    front = np.array([
    [0.5 * (sol[:n] @ (Q @ sol[:n])),   # ½ xᵀQx
     -(v @ sol[:n])]                    # –retorno
    for sol, _ in ND])
    nd_points [alpha] = front.copy()

end_total = time.perf_counter()
print(f"\nTempo total multi-start: {end_total - start_total:.3f}s")

time_conv = all_time.copy()
# =============================================================================
# TABELA 1: todas as soluções=
# =============================================================================
print("\nResumo (todas):")
print(" α | Retorno | Risco | Card | Dif_y | tau | NormaHad² | It.ext | Tempo")
print("-"*80)
for alpha in alpha_values:
    for sol,Fz,tau_v,diff_v,it_v,te in zip(all_solutions[alpha], all_F[alpha], all_tau[alpha], all_diff[alpha], all_iter[alpha], all_time[alpha]):
        x_f=sol[:n]
        print(f"{alpha:2}| {v@x_f:.6f}| {np.sqrt(x_f@(Q@x_f)):.6f}| {cardinalidade(x_f):4}| {binario(sol[n:]):4}| {tau_v:.4f}| {np.linalg.norm(x_f*sol[n:])**2:.6f}| {it_v:3}| {te:.4f}")

# ====================================================
# TABELA 2: soluções não dominadas AL
# ====================================================
print("\n α | Retorno | Risco | Card | Dif_y")
print("-"*50)
for alpha in alpha_values:
    for sol,Fz in all_ND[alpha]:
        x_f=sol[:n]
        print(f"{alpha:2}| {v@x_f:.6f}| {np.sqrt(x_f@(Q@x_f)):.6f}| {cardinalidade(x_f):4}| {binario(sol[n:]):4}")

# =========================================================
# 1° Gráfico extra: Todos os pontos AL (antes da filtragem)
# =========================================================
plt.figure(figsize=(8,6))
for alpha in alpha_values:
    # para cada solução (z) de cada α, plota o par (risco, retorno)
    for sol in all_solutions[alpha]:
        x_f = sol[:n]
        risco = np.sqrt(x_f @ (Q @ x_f))
        ret   = v @ x_f
        plt.scatter(risco, ret,
                    color=colors[alpha],
                    s=20,
                    alpha=0.5,
                    marker='o')
plt.xlabel('Risco')
plt.ylabel('Retorno')
plt.title('Risco vs Retorno - Todas as soluções AL (antes da filtragem)')
plt.grid(True)
plt.show()

# ====================================================
#  2° Gráfico extra: Risco x Retorno
# ====================================================
plt.figure(figsize=(8,6))
for alpha in alpha_values:
    for sol,_ in all_ND[alpha]:
        x_f = sol[:n]
        plt.scatter(np.sqrt(x_f@(Q@x_f)), v@x_f, color=colors[alpha], s=30, marker='o')
    plt.scatter([],[],color=colors[alpha],s=30,marker='o',label=f'AL α={alpha}')
plt.xlabel('Risco')
plt.ylabel('Retorno')
plt.title('Risco vs Retorno - AL')
plt.legend()
plt.grid(True)
plt.show()

# ============================================
# MIQP PONDERADO via CPLEX/DOcplex
# Informações sobre o método são descritas no trabalho
# ============================================

alpha_values_miqp = [1,2,4,6]
lambdas = np.linspace(0.0, 1, 100)
all_sols2 = []

def solve_weighted(alpha, lam):
    mdl = Model(name='moiqp')
    x = mdl.continuous_var_list(n, lb=0, ub=1, name='x')
    y = mdl.binary_var_list(n, name='y')
    mdl.add_constraint(mdl.sum(x) == 1)
    mdl.add_constraint(mdl.sum(y) >= n - alpha)
    for i in range(n): mdl.add_constraint(x[i] + y[i] <= 1)
    quad = mdl.sum(Q[i,j]*x[i]*x[j] for i in range(n) for j in range(n)) / 2
    lin  = mdl.sum(-v[i]*x[i] for i in range(n))
    mdl.minimize((1-lam)*quad + lam*lin)
    mdl.parameters.timelimit = 30
    t0 = time.time()
    sol = mdl.solve(log_output=False)
    t1 = time.time()
    if sol is None: return None
    xs = np.array([sol.get_value(xi) for xi in x])
    ys = np.array([sol.get_value(yi) for yi in y])
    f1 = float(xs@(Q@xs)/2)
    f2 = float(-v@xs)
    ok, msgs = verifica_viabilidade(np.concatenate([xs, ys]), n, alpha)
    return {'alpha':alpha,'lambda':lam,'f1':f1,'f2':f2,'risco':np.sqrt(xs@(Q@xs)),
            'ret':float(v@xs),'card':cardinalidade(xs),'viavel':ok,'msgs':msgs,'time':t1-t0}

print("\n====== MIQP CPLEX ======\n")
for alpha in alpha_values_miqp:
    print(f"--- α={alpha} ---")
    for lam in lambdas:
        sol = solve_weighted(alpha, lam)
        if sol is not None:
            print(f"α={sol['alpha']}, λ={sol['lambda']:.5f} -> f1={sol['f1']:.6f}, f2={sol['f2']:.6f}, card={sol['card']}, tempo={sol['time']:.3f}s, {'VIÁVEL' if sol['viavel'] else 'INVÁLIDO'}")
            all_sols2.append(sol)

#================= FILTRO PARETO MIQP =========================
pareto2 = {}

for α in alpha_values_miqp:
    cand = [s for s in all_sols2 if s['alpha'] == α]  # Só as soluções do alpha atual
    ND = []  # Lista auxiliar de soluções não dominadas para este alpha
    for s in cand:
        dominado = False
        F_orig = np.array([s['f1'], s['f2']])  # objetivos originais da solução s
        # primeiro laço: verifica se s é dominado por algum ponto já em ND
        for zp in ND:
            Fp = np.array([zp['f1'], zp['f2']])  # objetivos originais do ponto zp em ND
            if np.all(Fp <= F_orig) and np.any(Fp < F_orig):
                # zp domina s, então s é dominado
                dominado = True
                break
        if dominado:
            continue
        # segundo laço: remove do ND pontos que agora são dominados por s
        new_ND = []  # Nova lista sem pontos dominados
        for zp in ND:
            Fp = np.array([zp['f1'], zp['f2']])
            # Verifica se o ponto zp NÃO é dominado por s.
            if not (np.all(F_orig <= Fp) and np.any(F_orig < Fp)):
                # se zp não é dominado por s, mantemos ele no ND.
                new_ND.append(zp)
        # Como s não é dominado, adicionamos ele em ND.
        new_ND.append(s)
        # Atualiza o ND
        ND = new_ND
    # Guarda o ND encontrado
    pareto2[α] = ND

# Tabela Pareto MIQP

df_p = pd.concat(
    [pd.DataFrame(lst) for lst in pareto2.values()],
    ignore_index=True
)[ ['alpha','lambda','f1','f2','card','ret','risco','time'] ]
df_p.columns = ['α','λ','f₁','f₂','Card','Ret','Risco','Tempo(s)']
print("\n--- Soluções não-dominadas (Pareto) MIQP ---")
print(df_p.to_string(index=False, float_format="%.4f"))

# ====================================================
# PLOT COMBINADO: Risco x Retorno - AL vs MIQP
# ====================================================
# === PLOT COMBINADO CORRIGIDO ===
plt.figure(figsize=(8,6))

# Pontos AL (círculos), um scatter por α
for alpha in alpha_values:
    # extrai todos os x_f para este alpha
    pts_al = np.array([sol[:n] for sol, _ in all_ND[alpha]])
    riscos_al = np.sqrt((pts_al @ (Q @ pts_al.T)).diagonal())
    rets_al   =   (pts_al @ v)
    plt.scatter(riscos_al, rets_al,
                marker='o',
                color=colors[alpha],
                s=60,
                label=f'AL α={alpha}')

# Pontos MIQP (quadrados abertos), um scatter por α
for alpha in alpha_values_miqp:
    subset = pareto2.get(alpha, [])
    if not subset: continue
    arr = np.array([[s['risco'], s['ret']] for s in subset])
    plt.scatter(arr[:,0], arr[:,1],
                marker='s',
                facecolors='none',
                edgecolors = colors.get(alpha, 'grey'), #cor caso não esteja em colors
                s=80,
                linewidths=1.5,
                label=f'MIQP α={alpha}')

plt.title('Risco vs Retorno - AL vs MIQP')
plt.xlabel('Risco')
plt.ylabel('Retorno')
plt.legend()
plt.grid(True)
plt.show()

time_miqp = sum(s['time'] for lst in pareto2.values() for s in lst)

# =====================================================================================================
# CÁLCULO DE OUTRAS MÉTRICAS DESEMPENHO (NÃO USADO NA PRÁTICA-Tudo no notebook de perfis de desempenho)
#  Loop sobre cada α para calcular /HV vs kappa/ Delta - spread/ T_MIN (Não usado na prática)
# =====================================================================
ND_orig = {}
for α in alpha_values:
    # calcula todos os objetivos originais (f1,f2) = (1/2variância, -retorno)
    objs = [F_mobj(z) for z in all_solutions[α]]
    nd = []
    for o in objs:
        if not any((p[0] <= o[0] and p[1] <= o[1] and (p[0] < o[0] or p[1] < o[1]))
                   for p in nd):
            # remove dominados por o
            nd = [p for p in nd
                  if not ((o[0] <= p[0] and o[1] <= p[1]) and (o[0] < p[0] or o[1] < p[1]))]
            nd.append(o)
    ND_orig[α] = nd

import numpy as np
import pygmo as pg
hv_conv         = {}
spread_conv     = {}
time_min        = {}
for α in alpha_values:
    # monta não-dominados finais originais
    final_objs = np.array(ND_orig[α])
    # ponto de referência
    ref_global = final_objs.max(axis=0) + 1e-1 #ISSO É USADO APENAS NOS GRÁFICOS DE CONVERGÊNCIA DO HIPERV, NÃO EM NOSSA ANÁLISE
    hv_list         = []
    spread_list     = []
    partial = []
    for κ, z in enumerate(all_solutions[α], start=1):
        # acumula o κ-ésimo objetivo original
        o = F_mobj(z)
        partial.append(o)

        # filtra não-dominados parciais
        nd_part = []
        for p in partial:
            if not any((q[0] <= p[0] and q[1] <= p[1] and (q[0] < p[0] or q[1] < p[1]))
                       for q in nd_part):
                nd_part = [q for q in nd_part
                           if not ((p[0] <= q[0] and p[1] <= q[1]) and (p[0] < q[0] or p[1] < q[1]))]
                nd_part.append(p)

        # hypervolume parcial
        hv_list.append(pg.hypervolume(nd_part).compute(ref_global))

        # Δ-spread (Não usado na prática)
        A = np.array(nd_part)
        if len(A) < 2:
            spread_list.append(0.0)
        else:
            deltas = []
            for j in range(A.shape[1]):
                vj = np.sort(A[:, j]); d = vj[1:] - vj[:-1]
                m  = d.mean() if d.size>0 else 0.0
                num = d[0] + d[-1] + np.sum(np.abs(d - m))
                den = d[0] + d[-1] + len(d)*m
                deltas.append(num/den if den>0 else 0.0)
            spread_list.append(max(deltas))

    time_min[α]        = min(all_time[α])
    hv_conv[α]         = hv_list
    spread_conv[α]     = spread_list

# =====================================================
# Salva tudo em .npz para exportar ao notebook de perfis
# =====================================================
pen_type = "quad"
out_name = f'convergence_{pen_type}_n{n}.npz'
out_path = f'/content/drive/MyDrive/meus_experimentos2/{out_name}'
np.savez(out_path,
         hv_conv         = hv_conv,      # Não usado na prática (Convergencia do hipervolume)
         spread_conv     = spread_conv,  # Nao usado na prática (Métrica delta-Spread)
         time_min        = time_min,     # Não usado na prática
         time_conv       = time_conv,    # Adiciona todos os tempos por alpha
         nd_points       = nd_points,    # Todos os pontos não dominados encontrados
         alpha_values    = alpha_values, # Valores de alpha
         n                = n)           # A dimensão do problema
print(f"Convergence data salvo em {out_name} ")

# ========================================================
# TABELA RESUMO: ND AL-quad vs MIQP CPLEX
# ===========================================
print("\nResumo dos pontos não-dominados: AL-quad vs MIQP CPLEX")
print(" α | Método     | v_min   | v_max   | Q_min   | Q_max   | Card_min | Card_max")
print("--------------------------------------------------------------------------")

for alpha in alpha_values:
    # AL-quad
    fronteira_al = [(v @ z[:n], np.sqrt(z[:n] @ (Q @ z[:n])), cardinalidade(z[:n]))
             for z, _ in all_ND[alpha]]
    #  MIQP ----
    fronteira_miqp = [(s['ret'], s['risco'], s['card'])
               for s in pareto2.get(alpha, [])]

    for nome, fronteira in [("AL-quad", fronteira_al), ("MIQP", fronteira_miqp)]:
        if not fronteira:
            vmin = vmax = qmin = qmax = cmin = cmax = float('nan')
        else:
            rets, risks, cards = zip(*fronteira)
            vmin, vmax = min(rets),  max(rets)
            qmin, qmax = min(risks), max(risks)
            cmin, cmax = min(cards), max(cards)
        print(f"{alpha:2d} | {nome:10s} | {vmin:7.4f} | {vmax:7.4f} | "
              f"{qmin:7.4f} | {qmax:7.4f} | {cmin:9d} | {cmax:9d}")

#===============================================
# Código - CASO SIMPLES - 01/07 - AL-Exp
#===============================================
# Este script resolve o problema de otimização de portfólio multi‑objetivo
# para restrição de cardinalidade relaxada, usando o Lagrangeano Aumentado
# com penalidade exponencial (AL‑Exp).  Foram adicionados
# comentários em português para explicar o papel de cada função.

# Pré‑requisitos e montagem do Drive
from google.colab import drive  # Permite gravar e ler arquivos no Google Drive

drive.mount('/content/drive')# cria o drive em /content/drive  – só usado no Google Colab

# =============================================================================
# Instalação de dependências (execute no terminal, não dentro do script):
#  As linhas (!pip) abaixo servem apenas de lembrete do que precisa estar instalado.
!pip install numpy scipy matplotlib pandas cplex docplex
!pip install pygmo
# =============================================================================

# ======================Bibliotecas básicas ===================================
import numpy as np                     # Álgebra vetorial para pythom
from scipy.optimize import minimize    # SLSQP para sub‑problema de PGMMOP
import matplotlib.pyplot as plt        # Criação de gráficos
import time                            # Mensuração de tempo
import pandas as pd                    # Tabelas
import requests                        # Download de datasets
from docplex.mp.model import Model     # CPLEX/DOcplex p/ resolver MIQP
import pygmo as pg                     # Métricas multi‑objetivo (hipervolume)
# ============================================================================
# DADOS DO PORTFÓLIO
# ============================================================================
n = 6                                  # dimensão do problema
v = np.array([0.021, 0.04, -0.034, -0.028, -0.005, 0.006])
Q = np.array([
    [0.038, 0.020, 0.017, 0.014, 0.019, 0.017],
    [0.020, 0.043, 0.015, 0.013, 0.021, 0.014],
    [0.017, 0.015, 0.034, 0.011, 0.014, 0.014],
    [0.014, 0.013, 0.011, 0.044, 0.014, 0.011],
    [0.019, 0.021, 0.014, 0.014, 0.040, 0.014],
    [0.017, 0.014, 0.014, 0.011, 0.014, 0.046]
])
print("Dimensão do problema (n):", n)
print("\nVetor de retornos esperados (v):")
print(v)
print("\nMatriz de covariância (Q):")
print(Q)

colors = {1: 'red', 2: 'green', 3: 'orange', 4: 'magenta', 5: 'cyan', 6: 'black'}

# ============================================
# FUNÇÕES AUXILIARES
# ============================================

def tau_Rayleigh(Q):
    ones_vec = np.ones(Q.shape[0])
    t = Q @ ones_vec
    return (t.T @ (Q @ t)) / (t.T @ t + 1e-12)

def cardinalidade(x):
    return np.sum(x >= 1e-4)

def binario(yk):
    return np.sum((yk >= 1e-12) & (yk != 1.0))

def proj_box(x, low, up):
    return np.minimum(np.maximum(x, low), up)

def proj_hyperplane(x, w, cte):
    return x + ((cte - np.dot(w, x)) / (np.dot(w, w) + 1e-12)) * w

def proj_halfspace(x, w, cte):
    if np.dot(w, x) < cte:
        return x + ((cte - np.dot(w, x)) / (np.dot(w, w) + 1e-12)) * w
    else:
        return x

# ============================================
# MÉTODO DE DYKSTRA PARA (x,y) e PROJ
# Transcrito para python do trabalho de Krejic (2023)
# ============================================

def dykstra(x, y, ro, alfa, up, low, v, maxcycles=500, tol=1e-12):
    n_local = len(x)
    e_local = np.ones(n_local)
    dk2 = np.zeros(n_local); dk3 = np.zeros(n_local)
    dk5 = np.zeros(n_local); dk6 = np.zeros(n_local)
    err = 2.0; k = 0
    while err >= tol and k < maxcycles:
        T = 0.0
        # hiperplano soma(x)=1
        z = x - dk2
        x1 = proj_hyperplane(z, e_local, 1.0)
        t = x1 - z; T += np.linalg.norm(t - dk2)**2; dk2 = t
        # caixa x
        z = x1 - dk3
        x2 = proj_box(z, low, up)
        t = x2 - z; T += np.linalg.norm(t - dk3)**2; dk3 = t
        # semiespaço soma(y)>=n-alfa
        z = y - dk5
        y1 = proj_halfspace(z, e_local, n_local - alfa)
        t = y1 - z; T += np.linalg.norm(t - dk5)**2; dk5 = t
        # caixa y
        z = y1 - dk6
        y2 = proj_box(z, low, up)
        t = y2 - z; T += np.linalg.norm(t - dk6)**2; dk6 = t
        err = np.sqrt(T)
        x, y = x2, y2
        k += 1
    return x, y, err

def proj(z, v, low, up, ro, alfa):
    x = z[:n]; y = z[n:]
    xp, yp, _ = dykstra(x, y, ro, alfa, up, low, v)
    return np.concatenate([xp, yp])

def verifica_viabilidade(z, n, alfa, tol=1e-8):
    x = z[:n]; y = z[n:]
    ok = True; msgs = []
    if abs(x.sum() - 1.0) > tol:
        ok = False; msgs.append(f"Soma x={x.sum():.6f}≠1")
    if not (np.all(x>=-tol) and np.all(x<=1+tol)):
        ok = False; msgs.append("x fora [0,1]")
    if not (np.all(y>=-tol) and np.all(y<=1+tol)):
        ok = False; msgs.append("y fora [0,1]")
    if y.sum() < (n - alfa - tol):
        ok = False; msgs.append(f"Soma y={y.sum():.6f}<{n-alfa}")
    return ok, msgs

# ====================================================
# FUNÇÕES DO LAGRANGEANO AUMENTADO COM PENALIDADE EXPONENCIAL
# ====================================================
def F_mobj(z):
    """
    Retorna o vetor objetivo [f1, f2] onde:
      f1 = 0.5 * x^T Q x  e
      f2 = -v^T x.
    """
    x = z[:n]
    return np.array([0.5 * np.dot(x, Q @ x), -np.dot(v, x)])

import numpy as np

def lagrangiano_penalizado(z, tau, lam):
    """
    L(x,y,lamb,tau) = F_mobj(x) + [ lamb^T (x∘y)+ tau*sum(e^{(x_i y_i)^2} - 1)
                               + ||lamb||² / (2tau) ] · e,
    onde e = (1, 1)^T.
    Retorna vetor de dimensão 2 (mesmas penalizações somadas a f1 e f2).
    """
    x, y = z[:n], z[n:]
    base      = F_mobj(z)                                # [f1, f2]
    lin_term  = np.dot(lam, x * y)                       # lambda^T(xoy)
    pen_term  = tau * np.sum(np.exp((x * y)**2) - 1)     # tau sum (e^{(x_i y_i)^2}-1)
    quad_term = 0.5 * np.linalg.norm(lam)**2 / tau       # ||lamb||² / (2tau)

    extra = lin_term + pen_term + quad_term
    return base + extra


def jacobian_lagrangiano_penalizado(z, tau, lam):
    """
    Retorna a Jacobiana (2×2n) do Lagrangeano aumentado penalizado exponencial.
    """
    x = z[:n]
    y = z[n:]
    # Gradiente da parte quadrática f1 e f2
    # Derivada do termo exp
    pen_grad_x = 2 * tau * x * (y**2) * np.exp((x * y)**2)
    pen_grad_y = 2 * tau * (x**2) * y * np.exp((x * y)**2)
    # Gradiente de f1 em relação a x e y
    grad_f1_x = Q @ x + lam * y + pen_grad_x
    grad_f1_y = lam * x + pen_grad_y
    # Gradiente de f2 em relação a x e y
    grad_f2_x = -v + lam * y + pen_grad_x
    grad_f2_y = lam * x + pen_grad_y
    return np.vstack((np.concatenate((grad_f1_x, grad_f1_y)),
                      np.concatenate((grad_f2_x, grad_f2_y))))


def Armijo_tamanho_passo(z, d, F, J, A_k, delta):
    """
    Seleciona tamanho de passo t através do critério de Armijo não-monótono do tipo máximo..
    Parâmetros:
    z : Ponto atual (x,y)
    d : Direção de busca obtida pelo SLSQP
    F : Função objetivo penalizada (na prática "lagrangiano_penalizado"), que retorna vetor de dimensão 2.
    J : Avaliação do gradiente em z na direção d, isto é [∇L₁·d, ∇L₂·d].
    A_k : Par (A₁, A₂) acumulado nas últimas iterações.
    delta : Parâmetro usado apenas em Armijo.

    Retorna: O maior tamanho de passo t=1/2^j (j = 0, 1, ...,20) que satisfaz a cond. de Armijo
        F(z + td) ≤ A_k + δ t J;
        retorna 1.0 se nenhum j satisfizer.
    """
    for j in range(20):
        t_val = 1/(2**j)
        z_next = z + t_val*d
        F_next = np.array(F(z_next))
        rhs = np.array(A_k) + delta*t_val*J
        if np.all(F_next <= rhs):
            return t_val
    return 1

def compute_A_k(f_values, k, M):
    """
    Calcula o valor de A_k para Armijo não-monótono.

    Parâmetros usados:
    f_values : Histórico de valores de F(z) (cada um sendo um vetor de dimensão 2).
    k : índice da iteração atual.
    M : Número máximo de iterações consideradas em A_k (M=10).

    Retorna:
    A_k : Par (A₁, A₂), onde cada componente é o máximo de F_j nas últimas min(k, M) iterações.
    """
    m_k = min(k, M)
    recentes = f_values[max(0, k-m_k) : k+1] #Seleciona o conjunto de valores F(z^j) desde j = k–m_k até j = k
    if not recentes:  # Caso k=0 e f_values tem só um elemento
        return f_values[-1]
    A1 = max(f[0] for f in recentes)
    A2 = max(f[1] for f in recentes)
    return (A1, A2)

def processo_interno_PGMMOP(z, tau, lam, alpha, max_iter=50,M=10, delta=0.5, tol_inner=1e-08):
    """
    Usado para executar ométodo PGMMOP para o Lagrangiano aumentado.
    Parâmetros:
    z : Ponto inicial [x,y].
    tau : Parâmetro de penalização.
    lam : Vetor com Multiplicadores de Lagrange.
    alpha : Cardinalidade trabalhada.
    max_iter : Número máximo de iterações no processo interno.
    M : Usado em compute_A_k.
    delta : Usado na condição de Armijo.
    tol_inner: Tolerância para critério de parada interno (tol_1).

    Retorna:
    z_new : Ponto final (x,y).
    d : Última direção de busca encontrada.
    k : Número de iterações internas utilizadas.
    Fz_new : valor de F(z_new) ao final do processo.

    res=minimize é a função que utiliza SLSQP para encontrar a direção de busca com base nas restrições presentes em "cons"
    """
    f_vals = []
    for k in range(max_iter):
        Fzp = lagrangiano_penalizado(z, tau, lam)
        f_vals.append(Fzp)
        d0 = np.zeros_like(z)

      # Definimos os vetores gradientes em relação a x e y da função lagrangiana:
        # nova abordagem suave com epígrafe
        x, y = z[:n], z[n:]
        pen_grad_x = 2 * tau * x * (y**2) * np.exp((x * y)**2)
        pen_grad_y = 2 * tau * (x**2) * y * np.exp((x * y)**2)

        b1 = np.concatenate((Q @ x + lam * y + pen_grad_x,
                             lam * x + pen_grad_y))
        b2 = np.concatenate((-v + lam * y + pen_grad_x,
                              lam * x + pen_grad_y))

        # variável inicial do SLSQP: d concatenado com t=0:
        dt0 = np.concatenate([np.zeros_like(z), [0.0]])

        # função objetivo suave para determinar direção de busca
        def obj_suave(dt):
            d = dt[:-1]
            t = dt[-1]
            return t + 0.5 * np.dot(d, d)

        # Novo gradiente objetivo suave
        def obj_suave_grad(dt):
            d = dt[:-1]
            grad_d = d
            grad_t = 1.0
            return np.concatenate([grad_d, [grad_t]])

        # Restrições do problema original mais duas restrições novas para suavizar o máximo na busca de direção
        # Mais informações sobre o minimize e sua construção são encontradas no trabalho.
        # Os "lambda dt: <expressão com dt>" significa que criamos na hora uma função anonima com variação dt
        # Por exemplo, "lambda dt: np.sum(z[:n]+dt[:n]) - 1" seria c(dt)=sum(z_i+dt_i)-=0
        cons = [
            {"type":"eq",   "fun": lambda dt: np.sum(z[:n]+dt[:n]) - 1},
            {"type":"ineq", "fun": lambda dt: z[:n]+dt[:n]},
            {"type":"ineq", "fun": lambda dt: 1-(z[:n]+dt[:n])},
            {"type":"ineq", "fun": lambda dt: z[n:]+dt[n:2*n]},
            {"type":"ineq", "fun": lambda dt: 1-(z[n:]+dt[n:2*n])},
            {"type":"ineq", "fun": lambda dt: np.sum(z[n:]+dt[n:2*n]) - (n-alpha)},
            # restrições do suavização do máximo
            {'type':'ineq', 'fun': lambda dt: dt[-1] - np.dot(b1, dt[:-1])},
            {'type':'ineq', 'fun': lambda dt: dt[-1] - np.dot(b2, dt[:-1])}
        ]

        # Resolve o problema suave com SLSQP
        res = minimize(obj_suave, dt0, method='SLSQP',
                      jac=obj_suave_grad,
                      constraints=cons,
                      options={'ftol':1e-06, 'maxiter':500, 'disp':False})

        # Recupera a direção ótima d (descarta o valor ótimo t)
        d = res.x[:-1]
        Ak = compute_A_k(f_vals, k, M)
        Jv = np.array([
            jacobian_lagrangiano_penalizado(z,tau,lam)[0]@d,
            jacobian_lagrangiano_penalizado(z,tau,lam)[1]@d])
        a = Armijo_tamanho_passo(z, d, lambda zz: lagrangiano_penalizado(zz, tau, lam),
                                      Jv, Ak, delta)
        z_new = z + a*d
        z_new = proj(z_new, v, 0, 1, 0, alpha)
        if np.linalg.norm(d) <= tol_inner:
            return z_new, d, k, F_mobj(z_new)
        z = z_new
    return z, d, k, F_mobj(z_new)

# ============================================
# Processo externo da penalização
# ============================================

def processo_externo_penalizacao(z0, alpha, max_outer=50, tol_stop=1e-8):
    """
    Executa o loop externo de atualização de τ e λ para o Lagrangiano aumentado.

    Parâmetros:
    z0 : Ponto de partida [x, y].
    alpha : Cardinalidade trabalhada.
    max_outer : Quantidade máxima de iterações externas.
    tol_stop : Tolerância do critério de parada externo (tol_2).

    Retorna :
    z_new :Solução final.
    d : Última direção de busca interna.
    iter_externa : Quantidade de iterações externas usadas.
    Fz : Valor de F(z) da última iteração interna
    tau : Último parâmetro de penalização.
    norm_diff : Diferença ||F(z_new) – F_prev|| usada também no critério de parada.
    lam : Multiplicadores de Lagrange finais.
    t_ext : Tempo total usado nas iterações externas (em segundos).
    t_int : tempo total usado nas iterações internas (em segundos).
    """
    z = z0.copy()
    tau = tau_Rayleigh(Q)
    lam = np.zeros(n)
    lam_min, lam_max = -1e4, 1e4
    tau_max = 1e4
    eta_val = 10
    t_ext = t_int = 0.0

    # calcula F na partida e a norma do produto de Hadamard inicial
    F_prev = F_mobj(z)
    normHad_prev = np.linalg.norm(z[:n] * z[n:])

    for k in range(max_outer):
        t0 = time.time()
        # Usa o processo interno
        z_new, d, it_int, Fz = processo_interno_PGMMOP(z, tau, lam, alpha)
        t1 = time.time()
        t_int += (t1 - t0)

        x_new, y_new = z_new[:n], z_new[n:]
        norm_diff = np.linalg.norm(Fz - F_prev)
        normHad = np.linalg.norm(x_new * y_new)

        print(f"Iter. ex: {k} (it. int = {it_int}, ||d|| = {np.linalg.norm(d):.2e}): "
              f"tau = {tau:.7f}, lambda = {lam}, normHad = {normHad:.2e}, ||ΔF|| = {norm_diff:.2e}")

        # Critério de parada externo presente no trabalho
        cond1 = normHad**2 <= tol_stop*(np.linalg.norm(x_new)**2 * np.linalg.norm(y_new)**2)
        cond2 = norm_diff <= tol_stop
        if cond1 and cond2:
            t_ext += time.time() - t0
            print("Critério externo atingido.")
            return z_new, d, k+1, Fz, tau, norm_diff, lam, t_ext, t_int

        # Atualiza o vetor lam projetando em [lam_min, lam_max]^n
        lam = np.clip(lam + tau*(x_new * y_new), lam_min, lam_max)
        # Atualiza tau conforme regra apresentada nas observações do trabalho
        if normHad <= (1/eta_val) * normHad_prev:
            tau_new = tau
        else:
            eta_val += ((n - alpha)/(2*n))*(abs(v @ x_new) / (np.sqrt(x_new @ (Q @ x_new)) + 1e-12))
            tau_new = eta_val * tau
        tau = min(tau_new, tau_max)

        # Prepara próxima iteração
        F_prev, normHad_prev, z = Fz, normHad, z_new
        t_ext += time.time() - t0

    # Caso atinja o máximo de iterações programadas, retorna o seguinte
    return z, d, max_outer, Fz, tau, norm_diff, lam, t_ext, t_int

# ====================================================
# Parâmetros do multi-start
# ====================================================
n_starts     = 1
alpha_values = [1,2,4,6]   # ou os α que você de fato quiser testar

# ============================================
# GERAÇÃO DOS PONTOS INICIAIS (ESSA GERAÇÃO USANDO RAW_INITS DE PONTOS
#INICIAIS É USADA SOMENTE PARA APROXIMAÇÃO DA FRONTEIRA DE PARETO NOS PROBLEMAS
# DE N=6,31,85 PARA OBTER PONTOS NO CONJUNTO VIÁVEL APÓS DYKSTRA
# E para PORT3 (N=89) e PORT4 (N=98) USAMOS APENAS Z_CENTRO COMO PONTO INICIAL
# ============================================
np.random.seed(42)
raw_inits = []
z_centro = np.concatenate([np.full(n, 1/n), np.zeros(n)])
for _ in range(n_starts):
    # direção aleatória
    d = np.random.randn(2 * n)
    d /= np.linalg.norm(d)
    # raio uniforme em [0,1]
    rho = np.random.uniform(0, 10)
    # candidato sem projeção ainda
    raw_inits.append(z_centro + rho * d)

# ============================================
# LOOP PRINCIPAL POR α (MULTI-START)
# ============================================

# Dicionários vázios usados para guardarem informações para cada alpha em alpha_values
all_solutions = {};  # "all_solutions" lista dos vetores ótimos z* encontrados para cada start
all_F = {};          # "all_F" lista dos valores de F(z*) (o vetor de objetivos)
all_tau = {};        # "all_tau" lista de valores finais de τ
all_diff = {};       # "all_diff" lista de valores ΔF (mudança de F)
all_iter = {};       # "all_iter" lista de números de iterações externas usadas
all_time = {};       # "all_time" lista de tempos (s) gastos em cada run
all_ND_exp = {};         # "all_ND" lista de listas de soluções não‑dominadas encontradas
nd_points_exp = {};      # "nd_points" lista das frentes não‑dominadas F(z*)=(f₁(z*),f₂(z*))

start_total = time.perf_counter()

for alpha in alpha_values:
    print(f"\n===== Executando para α = {alpha} =====")

    # vetores de acumulação para este α
    sols = [];        # z* de cada start
    Fs = [];          # F(z*) de cada start
    taus = [];        # tau final de cada start
    diffs = [];       # ||ΔF|| de cada start
    iters_e = [];     # número de iters externas de cada start
    times_e = [];     # tempo de cada start
    ND = [];          # lista de (z*,F_orig) não-dominados parciais

    proj_inits = []
    for raw_z in raw_inits:
        z0 = raw_z.copy()
        proj_z = proj(z0, v, low=0, up=1, ro=0, alfa=alpha)
        proj_inits.append(proj_z)

    # =======================================================
    #  MULTI-START: roda o AL+SPG a partir de cada proj_z
    # =======================================================
    for i, z0 in enumerate(proj_inits, 1):
        print(f"\n--- Início {i} para α = {alpha} ---")

        (z_opt, d, it_ex, Fz, tau_f,
         diff_f, lam_f, t_ext, t_int) = processo_externo_penalizacao(z0, alpha)

        # armazena resultados
        sols .append(z_opt)
        Fs   .append(Fz)
        taus .append(tau_f)
        diffs.append(diff_f)
        iters_e.append(it_ex)
        times_e.append(t_ext)

        # impressão de diagnóstico
        print("Ponto inicial (projetado):", np.round(z0, 4))
        print("Ponto ótimo z*           :", np.round(z_opt, 4))
        vi, msgs = verifica_viabilidade(z_opt, n, alpha)
        print("VIÁVEL" if vi else f"NÃO VIÁVEL: {msgs}")

        x_f, y_f = z_opt[:n], z_opt[n:]
        print(f"Retorno {v @ x_f:.6f}, Risco {np.sqrt(x_f @ (Q @ x_f)):.6f}, "
              f"Card {cardinalidade(x_f)}, Dif_y {binario(y_f)}")

        # ===================================================
        #Atualiza lista de não-dominados (ND)
        # ===================================================
        # Calculamos os objetivos originais (f_1,f_2) para o ponto ótimo atual z_opt
        F_orig = F_mobj(z_opt)          # F(z_opt)=[f1(z_opt),f2(z_opt)]
        dominado = False               # Iniciamos tomando "dominado" como False (para indicar se o ponto ótimo é dominado)
        new_ND = []                    # lista auxiliar vazia para armazenar os pontos não dominados.
        # primeiro laço, verificamos se z_opt é dominado.
        for zp, Fp in ND: #Percorre todos os pontos da lista ND (não dominados existentes) e verifica se z_opt é dominado por algum ponto já em ND.
            # Fp <= F_orig é um vetor booleano (True ou False) com o resultado dessa comparação em todos (np.all) os componentes de f1 e f2.
            # Fp < F_orig cria outro vetor booleano agora para menor estrito e np.any verifica se existe ao menos uma componente com essa desigualdade.
            # and liga essas duas questões necessárias para a definição clássica para que zp seja considerado dominado.
            if np.all(Fp <= F_orig) and np.any(Fp < F_orig):
               # Se zp domina z_opt, então z_opt é dominado e podemos parar.
                dominado = True
                break
         # segundo laço, remoção dos pontos dominados e atualização da lista.
        for zp, Fp in ND: #remove do ND pontos que agora são dominados por z_opt
            if not (np.all(F_orig <= Fp) and np.any(F_orig < Fp)):
                new_ND.append((zp, Fp)) # Se zp não é dominado por F_orig, mantemos zp na lista
        if not dominado: #se z_opt não for dominado, adicionamos ele na lista de não dominados
            new_ND.append((z_opt, F_orig))
         # Atualiza a lista de não-dominados.
        ND = new_ND

    # guarda resultados deste α
    all_solutions[alpha] = sols
    all_F        [alpha] = Fs
    all_tau      [alpha] = taus
    all_diff     [alpha] = diffs
    all_iter     [alpha] = iters_e
    all_time     [alpha] = times_e
    all_ND_exp[alpha] = ND

    # fronteira de Pareto que será exportada
    front = np.array([
    [0.5 * (sol[:n] @ (Q @ sol[:n])),   # 1/2xQx
     -(v @ sol[:n])]                    # –ret
    for sol, _ in ND])
    nd_points_exp[alpha] = front.copy()

end_total = time.perf_counter()
print(f"\nTempo total multi-start: {end_total - start_total:.3f}s")

time_conv = all_time.copy()
# ====================================================
# TABELA 1: todas as soluções
# ====================================================
print("\nResumo (todas):")
print(" α | Retorno | Risco | Card | Dif_y | tau | NormaHad² | It.ext | Tempo")
print("-"*80)
for alpha in alpha_values:
    for sol,Fz,tau_v,diff_v,it_v,te in zip(all_solutions[alpha], all_F[alpha], all_tau[alpha], all_diff[alpha], all_iter[alpha], all_time[alpha]):
        x_f=sol[:n]
        print(f"{alpha:2}| {v@x_f:.6f}| {np.sqrt(x_f@(Q@x_f)):.6f}| {cardinalidade(x_f):4}| {binario(sol[n:]):4}| {tau_v:.4f}| {np.linalg.norm(x_f*sol[n:])**2:.6f}| {it_v:3}| {te:.4f}")

# ====================================================
# TABELA 2: soluções não dominadas AL
# ====================================================
print("\n α | Retorno | Risco | Card | Dif_y")
print("-"*50)
for alpha in alpha_values:
    for sol,Fz in all_ND_exp[alpha]:
        x_f=sol[:n]
        print(f"{alpha:2}| {v@x_f:.6f}| {np.sqrt(x_f@(Q@x_f)):.6f}| {cardinalidade(x_f):4}| {binario(sol[n:]):4}")

# ====================================================
# PLOT EXTRA: TODOS os pontos AL (antes da filtragem)
# ====================================================
plt.figure(figsize=(8,6))
for alpha in alpha_values:
    # para cada solução (z) de cada α, plota o par (risco, retorno)
    for sol in all_solutions[alpha]:
        x_f = sol[:n]
        risco = np.sqrt(x_f @ (Q @ x_f))
        ret   = v @ x_f
        plt.scatter(risco, ret,
                    color=colors[alpha],
                    s=20,
                    alpha=0.5,
                    marker='o')
plt.xlabel('Risco')
plt.ylabel('Retorno')
plt.title('Risco vs Retorno - Todas as soluções AL (antes da filtragem)')
plt.grid(True)
plt.show()

# ====================================================
# PLOT: Risco x Retorno (+ ND)
# ====================================================
plt.figure(figsize=(8,6))
for alpha in alpha_values:
    for sol,_ in all_ND_exp[alpha]:
        x_f = sol[:n]
        plt.scatter(np.sqrt(x_f@(Q@x_f)), v@x_f, color=colors[alpha], s=30, marker='o')
    plt.scatter([],[],color=colors[alpha],s=30,marker='o',label=f'AL α={alpha}')
plt.xlabel('Risco')
plt.ylabel('Retorno')
plt.title('Risco vs Retorno - AL')
plt.legend()
plt.grid(True)
plt.show()

# ============================================
# MIQP PONDERADO via CPLEX/DOcplex
# ============================================

alpha_values_miqp = [1,2,4,6]
lambdas = np.linspace(0.0, 1, 100)
all_sols2 = []

def solve_weighted(alpha, lam):
    mdl = Model(name='moiqp')
    x = mdl.continuous_var_list(n, lb=0, ub=1, name='x')
    y = mdl.binary_var_list(n, name='y')
    mdl.add_constraint(mdl.sum(x) == 1)
    mdl.add_constraint(mdl.sum(y) >= n - alpha)
    for i in range(n): mdl.add_constraint(x[i] + y[i] <= 1)
    quad = mdl.sum(Q[i,j]*x[i]*x[j] for i in range(n) for j in range(n)) / 2
    lin  = mdl.sum(-v[i]*x[i] for i in range(n))
    mdl.minimize((1-lam)*quad + lam*lin)
    mdl.parameters.timelimit = 30
    t0 = time.time()
    sol = mdl.solve(log_output=False)
    t1 = time.time()
    if sol is None: return None
    xs = np.array([sol.get_value(xi) for xi in x])
    ys = np.array([sol.get_value(yi) for yi in y])
    f1 = float(xs@(Q@xs)/2)
    f2 = float(-v@xs)
    ok, msgs = verifica_viabilidade(np.concatenate([xs, ys]), n, alpha)
    return {'alpha':alpha,'lambda':lam,'f1':f1,'f2':f2,'risco':np.sqrt(xs@(Q@xs)),
            'ret':float(v@xs),'card':cardinalidade(xs),'viavel':ok,'msgs':msgs,'time':t1-t0}

print("\n====== MIQP CPLEX ======\n")
for alpha in alpha_values_miqp:
    print(f"--- α={alpha} ---")
    for lam in lambdas:
        sol = solve_weighted(alpha, lam)
        if sol is not None:
            print(f"α={sol['alpha']}, λ={sol['lambda']:.5f} -> f1={sol['f1']:.6f}, f2={sol['f2']:.6f}, card={sol['card']}, tempo={sol['time']:.3f}s, {'VIÁVEL' if sol['viavel'] else 'INVÁLIDO'}")
            all_sols2.append(sol)

# =========== FILTRO PARETO MIQP ==================
pareto2 = {}

for α in alpha_values_miqp:
    cand = [s for s in all_sols2 if s['alpha'] == α]  # Só as soluções do alpha atual
    ND = []  # Lista auxiliar de soluções não dominadas para este alpha
    for s in cand:
        dominado = False
        F_orig = np.array([s['f1'], s['f2']])  # objetivos originais da solução s
        # primeiro laço: verifica se s é dominado por algum ponto já em ND
        for zp in ND:
            Fp = np.array([zp['f1'], zp['f2']])  # objetivos originais do ponto zp em ND
            if np.all(Fp <= F_orig) and np.any(Fp < F_orig):
                # zp domina s, então s é dominado
                dominado = True
                break
        if dominado:
            continue
        # segundo laço: remove do ND pontos que agora são dominados por s
        new_ND = []  # Nova lista sem pontos dominados
        for zp in ND:
            Fp = np.array([zp['f1'], zp['f2']])
            # Verifica se o ponto zp NÃO é dominado por s.
            if not (np.all(F_orig <= Fp) and np.any(F_orig < Fp)):
                # se zp não é dominado por s, mantemos ele no ND.
                new_ND.append(zp)
        # Como s não é dominado, adicionamos ele em ND.
        new_ND.append(s)
        # Atualiza o ND
        ND = new_ND
    # Guarda o ND encontrado
    pareto2[α] = ND


# Tabela Pareto MIQP

df_p = pd.concat(
    [pd.DataFrame(lst) for lst in pareto2.values()],
    ignore_index=True
)[ ['alpha','lambda','f1','f2','card','ret','risco','time'] ]
df_p.columns = ['α','λ','f₁','f₂','Card','Ret','Risco','Tempo(s)']
print("\n--- Soluções não-dominadas (Pareto) MIQP ---")
print(df_p.to_string(index=False, float_format="%.4f"))

# ====================================================
# PLOT COMBINADO: Risco x Retorno - AL vs MIQP
# ====================================================

plt.figure(figsize=(8,6))

# AL-exp  (círculos preenchidos)
for alpha in alpha_values:
    nd_alpha = all_ND_exp.get(alpha, [])         # lista de (z , Fz)
    if not nd_alpha:
        continue

    # transforma cada z em (risco , retorno)
    arr = np.array([[ np.sqrt(z[:n] @ (Q @ z[:n])),   # risco  (x)
                      v @ z[:n] ]                     # retorno (y)
                     for z, _ in nd_alpha])

    plt.scatter(arr[:,0], arr[:,1],
                marker='o',               # círculo cheio
                color=colors[alpha],
                s=60,
                label=f'AL-exp α={alpha}')

#  MIQP  (plota quadrados vazados)
for alpha in alpha_values_miqp:
    subset = pareto2.get(alpha, [])                # lista de dicionários
    if not subset:
        continue

    arr = np.array([[s['risco'], s['ret']]         # (x , y)
                    for s in subset])

    plt.scatter(arr[:,0], arr[:,1],
                marker='s', facecolors='none',      # quadrado vazio
                edgecolors=colors[alpha],
                s=80, linewidths=1.5,
                label=f'MIQP α={alpha}')
plt.title('Risco vs Retorno – AL-exp × MIQP')
plt.xlabel('Risco');  plt.ylabel('Retorno')
plt.legend();  plt.grid(True);  plt.tight_layout();  plt.show()

# =====================================================================================================
# CÁLCULO DE OUTRAS MÉTRICAS DESEMPENHO (NÃO USADO NA PRÁTICA-Tudo no notebook de perfis de desempenho)
#  Loop sobre cada α para calcular /HV vs kappa/ Delta - spread/ T_MIN (Não usado na prática)
# =====================================================================
import numpy as np
import pygmo as pg

ND_orig = {}
for α in alpha_values:
    # calcula todos os objetivos originais (f1,f2) = (1/2variancia, -retorno)
    objs = [F_mobj(z) for z in all_solutions[α]]
    nd = []
    for o in objs:
        if not any((p[0] <= o[0] and p[1] <= o[1] and (p[0] < o[0] or p[1] < o[1]))
                   for p in nd):
            # remove dominados por o
            nd = [p for p in nd
                  if not ((o[0] <= p[0] and o[1] <= p[1]) and (o[0] < p[0] or o[1] < p[1]))]
            nd.append(o)
    ND_orig[α] = nd


hv_conv         = {}
spread_conv     = {}
time_min        = {}
# ===================================================
#Loop sobre cada α para calcular HV, spread e Purity
# =====================================================
for α in alpha_values:
    # monta não-dominados finais originais
    final_objs = np.array(ND_orig[α])
    # ponto de referência
    ref_global = final_objs.max(axis=0) + 1e-1

    hv_list         = []
    spread_list     = []

    partial = []
    for κ, z in enumerate(all_solutions[α], start=1):
        # acumula o κ-ésimo objetivo original
        o = F_mobj(z)
        partial.append(o)

        # filtra não-dominados parciais
        nd_part = []
        for p in partial:
            if not any((q[0] <= p[0] and q[1] <= p[1] and (q[0] < p[0] or q[1] < p[1]))
                       for q in nd_part):
                nd_part = [q for q in nd_part
                           if not ((p[0] <= q[0] and p[1] <= q[1]) and (p[0] < q[0] or p[1] < q[1]))]
                nd_part.append(p)

        # hypervolume parcial
        hv_list.append(pg.hypervolume(nd_part).compute(ref_global))

        # Δ-spread
        A = np.array(nd_part)
        if len(A) < 2:
            spread_list.append(0.0)
        else:
            deltas = []
            for j in range(A.shape[1]):
                vj = np.sort(A[:, j]); d = vj[1:] - vj[:-1]
                m  = d.mean() if d.size>0 else 0.0
                num = d[0] + d[-1] + np.sum(np.abs(d - m))
                den = d[0] + d[-1] + len(d)*m
                deltas.append(num/den if den>0 else 0.0)
            spread_list.append(max(deltas))

    time_min[α]        = min(all_time[α])
    hv_conv[α]         = hv_list
    spread_conv[α]     = spread_list


# =====================================================
# Salva tudo em .npz
# =====================================================
pen_type = "exp"
out_name = f'convergence_{pen_type}_n{n}.npz'
out_path = f'/content/drive/MyDrive/meus_experimentos2/{out_name}'
np.savez(out_path,
         hv_conv         = hv_conv,
         spread_conv     = spread_conv,
         time_min        = time_min,
         time_conv       = time_conv,    # <- adiciona todos os tempos por start
         nd_points      = nd_points_exp,
         alpha_values    = alpha_values,
         n                = n)
print(f"Convergence data salvo em {out_name} ")


# =====================================================
# TABELA RESUMO: ND AL-exp vs MIQP CPLEX
# =====================================================
print("\nResumo dos pontos não-dominados: AL-exp vs MIQP CPLEX")
print(" α | Método     | v_min   | v_max   | Q_min   | Q_max   | Card_min | Card_max")
print("---------------------------------------------------------------------------")

for alpha in alpha_values:
    # ============= AL-exp ============
    fronteira_exp = [
        (v @ z[:n],                                      # retorno
         np.sqrt(z[:n] @ (Q @ z[:n])),                   # risco
         cardinalidade(z[:n]))                             # cardinalidade
        for z, _ in all_ND_exp[alpha]
    ]

    # ============ MIQP ================
    fronteira_miqp = [
        (s['ret'], s['risco'], s['card'])
        for s in pareto2.get(alpha, [])
    ]

    for nome, fronteira in [("AL-exp", fronteira_exp), ("MIQP", fronteira_miqp)]:
        if not fronteira:
            vmin = vmax = qmin = qmax = cmin = cmax = float('nan')
        else:
            rets, risks, cards = zip(*fronteira)
            vmin, vmax = min(rets),  max(rets)

            qmin, qmax = min(risks), max(risks)
            cmin, cmax = min(cards), max(cards)
        print(f"{alpha:2d} | {nome:10s} | {vmin:7.4f} | {vmax:7.4f} | "
              f"{qmin:7.4f} | {qmax:7.4f} | {cmin:9d} | {cmax:9d}")